{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@title: download the HCP resting-state dataset\n",
    "@version:0.1\n",
    "@author: Hui Zheng zh.dmtr@gmail.com\n",
    "@time: 2022-8-8 10:58:00\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "import boto3\n",
    "import json\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "import time\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_name = 'hcp-openaccess'\n",
    "s3_prefix = 'HCP_1200'\n",
    "access_key = ''\n",
    "secret_key = ''\n",
    "resource = boto3.resource('s3',aws_access_key_id=access_key,aws_secret_access_key=secret_key)\n",
    "bucket = resource.Bucket('hcp-openaccess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fold your want to download\n",
    "SERIES_MAP = {\n",
    "#'MEG_unprocessed':'unprocessed/MEG/',\n",
    "'3T_unprocessed_rfMRI1_LR':'unprocessed/3T/rfMRI_REST1_LR/',\n",
    "'3T_unprocessed_rfMRI1_RL':'unprocessed/3T/rfMRI_REST1_RL/',\n",
    "'3T_unprocessed_rfMRI2_LR':'unprocessed/3T/rfMRI_REST2_LR/',\n",
    "'3T_unprocessed_rfMRI2_RL':'unprocessed/3T/rfMRI_REST2_RL/',\n",
    "'3T_unprocessed_T1':'unprocessed/3T/T1w_MPR1/',\n",
    "#'7T_unprocessed':'7T',\n",
    "#'Diffusion':'Diffusion',\n",
    "#'T1w':'T1w',\n",
    "#'MNINonLinear':'MNINonLinear',\n",
    "'release-notes':'release-notes',\n",
    "#'MEG':'MEG'\n",
    "#'.xdlm':'.xdlm',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = Path.cwd()\n",
    "log_path = Path(project_path, \"log\")\n",
    "out_dir = './data/HCP/'\n",
    "t = time.strftime(\"%Y_%m_%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_md5(file):\n",
    "    if not os.path.isfile(file):\n",
    "        # return\n",
    "        raise Exception(\"The file:%s is not exist! Can't get md5 code!\"%file)\n",
    "    m = hashlib.md5()\n",
    "    with open(file, mode='rb') as f:\n",
    "        while True:\n",
    "            data = f.read(10240)\n",
    "            if not data:\n",
    "                break\n",
    "            m.update(data)\n",
    "    return m.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_integrity(file,md5_old):\n",
    "    md5 = get_file_md5(file)\n",
    "    if md5 == md5_old:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_subject(subject_path, divide_num):\n",
    "    subject_list = []\n",
    "    fr = open(subject_path, 'r', encoding='utf-8')\n",
    "    line = fr.readline()\n",
    "    while line:\n",
    "        subject_list.append(line.strip())\n",
    "        line = fr.readline()\n",
    "\n",
    "    task_subject = []\n",
    "    size = int(len(subject_list) /divide_num)\n",
    "    flag = False\n",
    "    if len(subject_list) % divide_num == 0:\n",
    "        for i in range(divide_num):\n",
    "            task_subject.append(subject_list[size * i:size * (i + 1)])\n",
    "    else:\n",
    "        for i in range(divide_num + 1):\n",
    "            task_subject.append(subject_list[size * i:size * (i + 1)])\n",
    "        flag = True\n",
    "    if flag:\n",
    "        task_subject[-2] += task_subject[-1]\n",
    "        task_subject.pop(-1)\n",
    "\n",
    "    return task_subject ## ruturn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_download(out_dir,subjects):\n",
    "    # resource = boto3.resource('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "    # bucket = resource.Bucket(s3_bucket_name)\n",
    "    # print('connect to client successfully!')\n",
    "    # logger.info(\"connect to client successfully!\")\n",
    "\n",
    "    for subject in subjects:\n",
    "        logger.add(f'{log_path}/{subject}_info_{t}.log', rotation=\"500MB\", encoding=\"utf-8\", enqueue=True, compression=\"zip\",\n",
    "                   retention=\"10 days\", level=\"INFO\")\n",
    "        logger.add(f'{log_path}/{subject}_error_{t}.log', rotation=\"500MB\", encoding=\"utf-8\", enqueue=True, compression=\"zip\",\n",
    "                   retention=\"10 days\",level=\"ERROR\")\n",
    "\n",
    "        time_start = time.time()\n",
    "\n",
    "        # read md5 file\n",
    "        fp = open('./data/md5/'+ str(subject) + '_md5.json', 'r', encoding='utf-8')\n",
    "        subject_md5 = json.loads(fp.readline())['Include']\n",
    "        fp.close()\n",
    "\n",
    "        s3_keys = bucket.objects.filter(Prefix='HCP_1200/%s/'%subject)\n",
    "        s3_keylist = [key.key for key in s3_keys]\n",
    "\n",
    "        prefixes = [\"HCP_1200/%s/%s\"%(subject,x) for x in SERIES_MAP.values()]\n",
    "        prefixes = tuple(prefixes)\n",
    "        s3_keylist = [x for x in s3_keylist if x.startswith(prefixes)]\n",
    "\n",
    "        # remove png and html\n",
    "        # s3_keylist = [x for x in s3_keylist if not x.endswith(('png','html'))]\n",
    "\n",
    "        # If output path doesn't exist, create it\n",
    "        if not os.path.exists(out_dir):\n",
    "            print('Could not find %s, creating now...' % out_dir)\n",
    "            logger.warning(f'Could not find {out_dir}, creating now...')\n",
    "            os.makedirs(out_dir)\n",
    "\n",
    "        total_num_files = len(s3_keylist)\n",
    "        files_downloaded = len(s3_keylist)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for path_idx, s3_path in enumerate(s3_keylist):\n",
    "            count += 1\n",
    "            rel_path = s3_path.replace(s3_prefix, '')\n",
    "            rel_path = rel_path.lstrip('/')\n",
    "\n",
    "            download_file = os.path.join(out_dir, rel_path)\n",
    "            download_dir = os.path.dirname(download_file)\n",
    "            # If downloaded file's directory doesn't exist, create it\n",
    "            if not os.path.exists(download_dir):\n",
    "                os.makedirs(download_dir)\n",
    "            try:\n",
    "                if not os.path.exists(download_file) or os.path.getsize(download_file) == 0:\n",
    "                    # while file is empty\n",
    "                    if os.path.exists(download_file):\n",
    "                        if os.path.getsize(download_file) == 0:\n",
    "                            print(\"%s is empty, download again!\" % (s3_path))\n",
    "                            logger.error(f\"{s3_path} is empty, download again!\")\n",
    "                    print('Downloading to: %s' % download_file)\n",
    "                    # with open(download_file, 'wb') as f:\n",
    "                    #     # download_file:  The path to the file to download to.\n",
    "                    #     # s3_path: The name of the key to download from.\n",
    "                    #     bucket.download_file(s3_path,download_file)\n",
    "                    bucket.download_file(s3_path,download_file)\n",
    "\n",
    "                    #md5 integrity verify, Waiting for optimization\n",
    "                    md5_old = None\n",
    "                    for item in subject_md5:\n",
    "                       if rel_path == item['URI']:\n",
    "                           md5_old = item['Checksum']\n",
    "                    if md5_old:\n",
    "                       verify = check_integrity(download_file,md5_old)\n",
    "                       if not verify:\n",
    "                           print(\"Download fail about file: %s\"%(s3_path))\n",
    "                           logger.error(f\"Download fail about file: {s3_path}\")\n",
    "                           # download fail, write to a file\n",
    "                           fw2 = open('./data/fail/'+str(subject)+'_fail.txt','a',encoding='utf-8')\n",
    "                           fw2.write(s3_path + '\\n')\n",
    "                           fw2.close()\n",
    "                    else:\n",
    "                       print(\"There are not have md5 code about the file: %s\" % (s3_path))\n",
    "                       logger.error(f\"There are have not md5 code about the file: {s3_path}\")\n",
    "\n",
    "                    print(\"FACTS: path: %s, file: %s\"%(s3_path, download_file))\n",
    "                    print('%.3f%% percent complete' % \\\n",
    "                          (100*(float(path_idx+1)/total_num_files)))\n",
    "                    complete_percent = (100*(float(path_idx+1)/total_num_files))\n",
    "                    if count%500 == 0:\n",
    "                        logger.info(f\"{complete_percent} percent complete\")\n",
    "                else:\n",
    "                    print('File %s already exists, skipping...' % download_file)\n",
    "                    files_downloaded -= 1\n",
    "            except Exception as exc:\n",
    "                print('There was a problem downloading %s.\\n'\\\n",
    "                      'Check and try again.' % s3_path)\n",
    "                logger.error(f\"There was a problem downloading {s3_path}. Check and try again.\")\n",
    "                print(exc)\n",
    "                logger.error(exc)\n",
    "\n",
    "        print('%d files downloaded for subject %s.' % (files_downloaded,subject))\n",
    "        logger.info(f\"{files_downloaded} files downloaded for subject {subject}.\")\n",
    "\n",
    "        print('Done!')\n",
    "        logger.info(\"DOne!\")\n",
    "\n",
    "        time_cost = (time.time()-time_start)/3600\n",
    "        print(\"Time cost of downloading {} is :{} h\".format(subject,time_cost))\n",
    "        logger.info(f\"Time cost of downloading {subject} is :{time_cost} h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl=divide_subject(\"./utils/subjects_want.txt\", 4)\n",
    "sl[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import sleep, ctime\n",
    "t1=Thread(target=collect_and_download,args=(out_dir,sl[0]))\n",
    "t2=Thread(target=collect_and_download,args=(out_dir,sl[1]))\n",
    "t3=Thread(target=collect_and_download,args=(out_dir,sl[2]))\n",
    "t4=Thread(target=collect_and_download,args=(out_dir,sl[3]))\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "t3.start()\n",
    "t4.start()\n",
    "t1.join()\n",
    "t2.join()\n",
    "t3.join()\n",
    "t4.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
